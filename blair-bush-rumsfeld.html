<!doctype html>
<html>
<head>
	<title>Blair, Bush or Rumsfeld</title>
	<style type="text/css">body {background-color: #ffeecc;}
	</style>
</head>
<body>
<h1>Blair, Bush or Rumsfeld?</h1>

<h3>Content</h3>

<ol>
	<li>Project Definition</li>
	<li>Training Data</li>
	<li>Convolutional Neural Network</li>
	<li>Model Training</li>
	<li>First Training Result</li>
	<li>Training Result with Data Shuffling</li>
	<li>Test Accuracy</li>
	<li>Data Augmentation</li>
	<li>Extending the Architecture</li>
	<li>Test Accuracy with Data Augmentation</li>
	<li>Detailed Study of the Result</li>
</ol>

<h3>1. Project Definition</h3>

<p>This project explores the development of a deep convolutional neural network (CNN) model for facial recognition given images of Tony Blair, George W Bush or Donald Rumsfeld. This is a closed set classification problem. The deep learning framework used is Keras running on TensorFlow backend.</p>

<h3>2. Training Data</h3>

<p>The data for training the model comes from the <em>Labeled Faces in the Wild</em> (hereafter LFW) <a href="http://vis-www.cs.umass.edu/lfw/" target="_blank">dataset</a>. This dataset&nbsp;contains more than 13,000 images of faces collected from the web, each face labeled with the name of the person pictured. Each image is 250&times;250 pixels. The three personalities in this project have been selected based on them all being caucasian males to provide added challenge and also for each having more than a hundred images (Blair 144; Bush 530; Rumsfeld 121) for training. A sampling of the images available for <em>Tony Blair</em> is shown below.</p>

<p><img alt="" src="https://github.com/basilhan/bbr/blob/master/images/tony-blair-lfw-orig.png?raw=true" /></p>

<p><small>Figure 1 : Original Labeled Faces in the Wild images.</small></p>

<p>As they are, the original images are useful in providing a variation of facial angles and expressions. However, they also contain a fair amount of noise - features that are irrelevant or potentially misleading for identifying a person. These include :</p>

<ol>
	<li>The background.</li>
	<li>Other persons.</li>
	<li>Hairstyle (just a glance above shows that Blair&#39;s hair can look quite different on different occasions).</li>
	<li>Clothes.</li>
	<li>Color.</li>
</ol>

<p>In order to maximize the amount of useful features contained within an image, it is desirable to crop away anything above the eyebrows and below the chin, and then only retain the remaining facial image between the ears. Thankfully, such a cropped derivative dataset is already available <a href="http://conradsanderson.id.au/lfwcrop/" target="_blank">online</a>. Each image in this dataset is 64&times;64 pixels.</p>

<p>The greyscale version of this dataset is employed. A sampling of the images for <em>Tony Blair</em> corresponding to the previous diagram is shown below.</p>

<p><img alt="" src="https://github.com/basilhan/bbr/blob/master/images/tony-blair-lfw-cropped.png?raw=true" /></p>

<p><font size="2">Figure 2 : Cropped and greyscaled Labeled Faces in the Wild images.</font></p>

<p>As an artifical of images for each person (class) is restricted to 100 for training and 10 for validation. Therefore, a total of 330 images (110 for each class) is used in the training step. The images are chosen based on running number from 1 to 110 in each class.</p>

<h3>3. Convolutional Neural Network</h3>

<p>The CNN is the preferred neural network architecture of <a href="http://www.iro.umontreal.ca/~bengioy/talks/DL-Tutorial-NIPS2015-vision.pdf" target="_blank">choice</a> for machine learning of image data. For this project, we start off with a network having 3 convolutional/pooling layers and 2 fully connected (FC) layers. The implementation in Keras code is shown below.</p>

<p><textarea cols="82" name="Keras Model Code" rows="13">model = models.Sequential()
model.add(layers.Conv2D(128, (3, 3), activation=&#39;relu&#39;, input_shape=input_shape))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(256, (3, 3), activation=&#39;relu&#39;))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation=&#39;relu&#39;))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(512, activation=&#39;relu&#39;))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(num_outputs, activation=final_activation))</textarea></p>

<p>In total, there are&nbsp;2,952,835 trainable parameters.</p>

<p><textarea cols="70" name="Keras Model Summary" rows="29">_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 62, 62, 128)       1280      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 31, 31, 128)       0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 29, 29, 256)       295168    
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 14, 14, 256)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 12, 12, 128)       295040    
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4608)              0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 4608)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               2359808   
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 3)                 1539      
=================================================================
Total params: 2,952,835
Trainable params: 2,952,835
Non-trainable params: 0</textarea></p>

<h4>Input Layer</h4>

<p>The greyscale training images come in dimensions of 64&times;64 pixels. Therefore, the input layer is of shape (64, 64, 1).</p>

<h4>Convolution/Pooling Layers</h4>

<p>There are 3 convolutional/pooling layers.</p>

<p>The number of feature maps generated are 128, 256 and 128.</p>

<p>The convolutional filters are all 3&times;3 and of default stride of 1.</p>

<p>The pooling windows are all 2&times;2 and of default stride of 2.</p>

<h4>Fully Connected Layers</h4>

<p>There are 2 FC layers, including the output layer.</p>

<p>The first FC layer contains 512 nodes.</p>

<p>The second FC (output) layer contains 3 nodes, since there are 3 classes.</p>

<h4>Activation Function</h4>

<p>The ReLU function is used for all activations for its proven effectiveness. The exception is the output layer, which makes use of the Softmax function for multi-class classification.</p>

<h4>Further References</h4>

<ul>
	<li><a href="https://cs231n.github.io/convolutional-networks/" target="_blank">Notes from CS231n : Convolutional Neural Networks for Facial Recognition</a></li>
</ul>

<h3>4. Model Training</h3>

<p>The model is trained on the categorical cross-entropy loss function using the RMSprop gradient descent optimization algorithm. To provide regularization, a dropout ratio of 0.5 is introduced in the final two fully connected layers. During training, a batch size of 10 is employed. The complete code listing is <a href="https://github.com/basilhan/bbr/blob/master/code/train.py" target="_blank">here</a>.</p>

<h3>5. First Training Result</h3>

<p>The result of training the model for 1,000 epochs is shown below.&nbsp;</p>

<p><img alt="" src="https://github.com/basilhan/bbr/blob/master/plots/bbr_all_not_shuffled_not_augmented_accuracy.jpg?raw=true" /></p>

<p><font size="2">Figure 3 : Training and validation accuracy.</font></p>

<p><img alt="" src="https://github.com/basilhan/bbr/blob/master/plots/bbr_all_not_shuffled_not_augmented_loss.jpg?raw=true" /></p>

<p><font size="2">Figure 4 : Training and validation loss.</font></p>

<p>For both the accuracy and loss, training appears to make no more progress beyond 300 epochs. Overfitting is clearly evident.</p>

<p>Since the training and validation data has been obtained by simply taking the first 100 and next 10 images in the LFW dataset, there is a possibility that there is an unknown bias at work. Therefore, the model is trained again with the images shuffled first before being partitioned into training and validation data.</p>

<h3>6. Training Result with Data Shuffling</h3>

<p>The result confirms that shuffling the images improves the result as shown below.</p>

<p><img alt="" src="https://github.com/basilhan/bbr/blob/master/plots/bbr_all_shuffled_not_augmented_accuracy.jpg?raw=true" /></p>

<p><font size="2">Figure 5 : Training and validation accuracy after image shuffling.</font></p>

<p>Compare this with Figure 3.</p>

<p><img alt="" src="https://github.com/basilhan/bbr/blob/master/plots/bbr_all_shuffled_not_augmented_loss.jpg?raw=true" /></p>

<p><font size="2">Figure 6 : Training and validation accuracy after image shuffling.</font></p>

<p>Compare this with Figure 4.</p>

<p>Since the result based on a dataset that has been shuffled prior to its partitioning into training and validation sets, this arrangement is used for further study.</p>

<p>The final model (<span style="font-family:courier new,courier,monospace;">bl_do2_rmsprop_gpu_20171027_1012-029-1.00â€‹*</span>) produced by the training is uploaded <a href="https://github.com/basilhan/bbr/tree/master/models" target="_blank">here</a>. It is about 23MB.</p>

<h3>7. Test Accuracy</h3>

<h4>Bing Search</h4>

<p>The models are fed with test images obtained using Bing image search. Having the test data from a different source to LFW will minimize any undesired correlations with the training data and help ensure a more realistic accuracy score.</p>

<p>The script used to perform an automated batch download of Bing search images can be found <a href="https://github.com/ostrolucky/Bulk-Bing-Image-downloader" target="_blank">here</a>. A sampling of the result returned for <em>Tony Blair</em> is shown below.</p>

<p><img alt="" src="https://github.com/basilhan/bbr/blob/master/images/tony-blair-bing.png?raw=true" /></p>

<p><font size="2">Figure 7</font></p>

<p>constraint, the number</p>

<h4>Image Cleaning</h4>

<p>Full details of the process can be found here.</p>

<p><img alt="" src="https://github.com/basilhan/bbr/blob/master/images/tony-blair-bing-cleaned.png?raw=true" /></p>

<h4>Testing Result</h4>

<p>The&nbsp;Keras <a href="https://keras.io/preprocessing/image/" target="_blank"><span style="font-family:courier new,courier,monospace;">ImageDataGenerator</span></a> class with 3 different random seeds is used to generate the test data samples for accuracy evaluation. The code listing is <a href="https://github.com/basilhan/bbr/blob/master/code/evaluate.py" target="_blank">here</a>. The following accuracies are obtained.</p>

<table border="2" style="width: 189.7px; height: 107px;">
	<tbody>
		<tr style="height: 18px;">
			<td style="width: 10px; height: 18px; text-align: center;"><strong>Seed</strong></td>
			<td style="width: 10px; height: 18px; text-align: center;"><strong>Accuracy</strong></td>
		</tr>
		<tr style="height: 18px;">
			<td style="width: 10px; height: 18px; text-align: center;">10</td>
			<td style="width: 10px; height: 18px; text-align: center;">0.623</td>
		</tr>
		<tr style="height: 18px;">
			<td style="width: 10px; height: 18px; text-align: center;"><span style="color: #333333;">17</span></td>
			<td style="width: 10px; height: 18px; text-align: center;"><span style="color: #333333;">0.608</span></td>
		</tr>
		<tr style="height: 18px;">
			<td style="width: 10px; height: 18px; text-align: center;">55</td>
			<td style="width: 10px; height: 18px; text-align: center;">0.606</td>
		</tr>
	</tbody>
</table>

<p>These figures are clearly far from satisfactory. Some enhancements are explored next.</p>

<h3>8. Data Augmentation</h3>

<p>There are a few possibilities to improve upon the initial result. The neural network architecture could be enhanced with more layers and/or feature maps. The data used for training could also be enhanced. A well-established technique, almost&nbsp;de rigueur, for the latter without the need for more data (i.e. images) is data augmentation. This is what should be tried first. The basic idea is to perform transformations on an original image to yield several minor variants. These transformations include shifts, rotations and shears. For non-faces, flips are also possible.</p>

<p>The Keras <span style="font-family:courier new,courier,monospace;">ImageDataGenerator</span> class is used to produce augmented training images. The transformation specifications are available <a href="https://github.com/basilhan/bbr/blob/master/code/bbr_data_augmentation_01.py" target="_blank">here</a>. Several examples generated for the same image of <em>Tony Blair</em> is shown below.</p>

<p><img alt="" src="https://github.com/basilhan/bbr/blob/master/images/tony-blair-image-augmentation.png?raw=true" /></p>

<p><font size="2">Figure</font></p>

<p>Re-training the model using data augmentation produces the results shown below.</p>

<p>The final model (<span style="font-family:courier new,courier,monospace;">bl_do2_rmsprop_gpu_20171027_1025-029-1.00*</span>) produced by the training is uploaded <a href="https://github.com/basilhan/bbr/tree/master/models" target="_blank">here</a>. It is about 23MB (same as previous).</p>

<p>&nbsp;</p>

<p><img alt="" src="https://github.com/basilhan/bbr/blob/master/plots/bbr_all_shuffled_augmented_accuracy.jpg?raw=true" /></p>

<p><font size="2">Figure</font></p>

<p><img alt="" src="https://github.com/basilhan/bbr/blob/master/plots/bbr_all_shuffled_augmented_loss.jpg?raw=true" /></p>

<p><font size="2">Figure</font></p>

<p>With more data to train, progress in the accuracy and loss is slower, as expected.</p>

<p>&nbsp;</p>

<h3>9. Extending the Architecture</h3>

<p>Before proceeding to evaluate the result of data augmentation, the effects of adding layers and feature maps in the CNN is also explored.</p>

<h4>Deepening the Network</h4>

<p>The number of trainable parameters decreases to 1,445,763.</p>

<p>The results are shown below.</p>

<p><img alt="" src="https://github.com/basilhan/bbr/blob/master/plots/bbr_all_shuffled_augmented_deeper_accuracy.jpg?raw=true" /></p>

<p><font size="2">Figure</font></p>

<p><img alt="" src="https://github.com/basilhan/bbr/blob/master/plots/bbr_all_shuffled_augmented_deeper_loss.jpg?raw=true" /></p>

<p><font size="2">Figure</font></p>

<p>The final model (<span style="font-family:courier new,courier,monospace;">bl_do2_rmsprop_gpu_20171031_0734-029-0.90*</span>) produced by the training is uploaded <a href="https://github.com/basilhan/bbr/tree/master/models" target="_blank">here</a>. It is about 11MB.</p>

<h4>Widening the Network</h4>

<p>The number of trainable parameters increases to 7,083,267.</p>

<p>The final model (<span style="font-family:courier new,courier,monospace;">bl_do2_rmsprop_gpu_20171031_0804-029-1.00*</span>) produced by the training is uploaded <a href="https://github.com/basilhan/bbr/tree/master/models" target="_blank">here</a>. It is about 55MB.</p>

<h4>&nbsp;</h4>

<h3>10. Testing Accuracy with Data Augmentation</h3>

<p>&nbsp;</p>

<p>&nbsp;</p>

<p>Here is a summary of the result thus far.</p>

<table border="2" style="width: 612px; height: 107px;">
	<tbody>
		<tr style="height: 18px;">
			<td style="width: 460.63px; height: 18px; text-align: center;"><strong>Description</strong></td>
			<td style="width: 98.37px; height: 18px; text-align: center;"><strong>Seed</strong></td>
			<td style="width: 116px; height: 18px; text-align: center;"><strong>Accuracy</strong></td>
			<td style="width: 77px; height: 18px; text-align: center;"><strong>Parameters</strong></td>
			<td style="width: 130px; height: 18px; text-align: center;"><strong>Size (MB)</strong></td>
		</tr>
		<tr style="height: 18px;">
			<td style="width: 460.63px; height: 18px;">&nbsp;Data augmentation, wider NN</td>
			<td rowspan="4" style="width: 98.37px; height: 72px; text-align: center;">10</td>
			<td style="width: 116px; height: 18px; text-align: center;">0.922</td>
			<td style="width: 77px; height: 18px; text-align: center;">7,083,267</td>
			<td style="width: 130px; height: 18px; text-align: center;">55</td>
		</tr>
		<tr style="height: 18px;">
			<td style="width: 460.63px; height: 18px;">&nbsp;Data augmentation, deeper NN</td>
			<td style="width: 116px; height: 18px; text-align: center;">0.904</td>
			<td style="width: 116px; height: 18px; text-align: center;">1,445,763</td>
			<td style="width: 130px; height: 18px; text-align: center;">11</td>
		</tr>
		<tr style="height: 18px;">
			<td style="width: 460.63px; height: 18px;">&nbsp;Data augmentation</td>
			<td style="width: 116px; height: 18px; text-align: center;">0.911</td>
			<td style="width: 116px; height: 18px; text-align: center;">2,952,835</td>
			<td style="width: 130px; height: 18px; text-align: center;">23</td>
		</tr>
		<tr style="height: 18px;">
			<td style="width: 460.63px; height: 18px;">&nbsp;No data augmentation</td>
			<td style="width: 116px; height: 18px; text-align: center;">0.623</td>
			<td style="width: 116px; height: 18px; text-align: center;">2,952,835</td>
			<td style="width: 130px; height: 18px; text-align: center;">23</td>
		</tr>
		<tr style="height: 18px;">
			<td style="width: 460.63px; height: 18px;"><span style="color: #808080;">&nbsp;Data augmentation, wider NN</span></td>
			<td rowspan="4" style="width: 98.37px; height: 61.37px; text-align: center;"><span style="color: #808080;">17</span></td>
			<td style="width: 116px; height: 18px; text-align: center;"><span style="color: #808080;">0.902</span></td>
			<td style="width: 77px; height: 18px; text-align: center;">7,083,267</td>
			<td style="width: 130px; height: 18px; text-align: center;"><span style="color: #999999;">55</span></td>
		</tr>
		<tr style="height: 18px;">
			<td style="width: 460.63px; height: 18px;"><span style="color: #808080;">&nbsp;Data augmentation, deeper NN</span></td>
			<td style="width: 116px; height: 18px; text-align: center;"><span style="color: #808080;">0.887</span></td>
			<td style="width: 116px; height: 18px; text-align: center;"><span style="color: #808080;">1,445,763</span></td>
			<td style="width: 116px; height: 18px; text-align: center;"><span style="color: #999999;">11</span></td>
		</tr>
		<tr style="height: 7.36px;">
			<td style="width: 460.63px; height: 7.36px;"><span style="color: #808080;">&nbsp;Data augmentation</span></td>
			<td style="width: 116px; height: 7.36px; text-align: center;"><span style="color: #808080;">0.894</span></td>
			<td style="width: 116px; height: 7.36px; text-align: center;"><span style="color: #808080;">2,952,835 </span></td>
			<td style="width: 116px; height: 7.36px; text-align: center;"><span style="color: #999999;">23</span></td>
		</tr>
		<tr style="height: 18px;">
			<td style="width: 460.63px; height: 18px;"><span style="color: #808080;">&nbsp;No data augmentation</span></td>
			<td style="width: 116px; height: 18px; text-align: center;"><span style="color: #808080;">0.608</span></td>
			<td style="width: 116px; height: 18px; text-align: center;"><span style="color: #808080;">2,952,835 </span></td>
			<td style="width: 116px; height: 18px; text-align: center;"><span style="color: #999999;">23</span></td>
		</tr>
		<tr style="height: 18px;">
			<td style="width: 460.63px; height: 18px;">&nbsp;Data augmentation, wider NN</td>
			<td rowspan="4" style="width: 98.37px; height: 72px; text-align: center;">55</td>
			<td style="width: 116px; height: 18px; text-align: center;">0.922</td>
			<td style="width: 77px; height: 18px; text-align: center;">7,083,267</td>
			<td style="width: 130px; height: 18px; text-align: center;">55</td>
		</tr>
		<tr style="height: 18px;">
			<td style="width: 460.63px; height: 18px;">&nbsp;Data augmentation, deeper NN</td>
			<td style="width: 116px; height: 18px; text-align: center;">0.886</td>
			<td style="width: 116px; height: 18px; text-align: center;">1,445,763</td>
			<td style="width: 116px; height: 18px; text-align: center;">11</td>
		</tr>
		<tr style="height: 18px;">
			<td style="width: 460.63px; height: 18px;">&nbsp;Data augmentation</td>
			<td style="width: 116px; height: 18px; text-align: center;">0.901</td>
			<td style="width: 116px; height: 18px; text-align: center;">2,952,835</td>
			<td style="width: 116px; height: 18px; text-align: center;">23</td>
		</tr>
		<tr style="height: 18px;">
			<td style="width: 460.63px; height: 18px;">&nbsp;No data augmentation</td>
			<td style="width: 116px; height: 18px; text-align: center;">0.606</td>
			<td style="width: 116px; height: 18px; text-align: center;">2,952,835</td>
			<td style="width: 116px; height: 18px; text-align: center;">23</td>
		</tr>
	</tbody>
</table>

<p>Below is a graphical summary of the model accuracy for the final 30 models (i.e. generated in epochs 970 to 999) in each of the 4 cases.</p>

<p><img alt="" src="https://github.com/basilhan/bbr/blob/master/plots/test_accuracy_01.jpg?raw=true" /></p>

<h3>11. Detailed Study of the Result</h3>

<p>&nbsp;</p>

<p>The use of <a href="http://cs231n.github.io/transfer-learning/" target="_blank">transfer learning</a>.</p>
</body>
</html>
